## INTRODUCTION - 09:30 to 10:30 - (Prof. C.V. Jawahar)

.Talked about significant problems in the field, and how various methods are being developed to deal with them. Domain adaptation is a problem where the testing data has a different distribution as compared to the training data. A different distribution in training data may arise due to different sampling methods, different modes of data collection, or different testing conditions. This limits the robustness of many commercial algorithms as seen in self driving cars and even Google Duplex. Data gathered for self driving cars is often under mild conditions, (eg. in daytime, when the weather is clear) and this causes the algorithm to underperform in new conditions. 

.A hypothetical, and very amusing Duplex conversation in India may go like [this](https://www.youtube.com/watch?v=QY-BfXXLSZ8), if not accounting for a real world environment and having conversation in a controlled environment as shown in the Google keynote.

.Talked about traditional methods used in computer vison where we tried to use generalized algorithms like SIFT, Gabor filters and HOGs for pattern recognition. Methods like Energy Minimization and super pixels were also used. 

.Datasets used for benchmarking improved in size, complexity and tasks that must be performed using them: Caltech101 (2003) was used for classification. PASCAL-VOC (2005) is still being used for classification, detection and segmentation.

.These methods were replaced by deep learning algorithms which given data taught itself to look for specific features and modify filters accordingly. AlexNet wins the 2012 ImageNet challenge and starts the Deep Learning based approach. 

.Modern problems such as variation of data at domain level and at classification level.

.
